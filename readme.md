## ðŸ“Š Real-Time & Batch Data Pipeline â€” Home Assessment

### ðŸ§­ Overview

This project demonstrates a hybrid **real-time + batch data processing pipeline** using modern data engineering tools and Docker-based orchestration. It includes:

- **Real-time ingestion** via RabbitMQ (producer/consumer pattern)
- **Batch processing** with Spark + Delta Lake triggered via Airflow or Jupyter
- **Data lake emulation** via S3Mock (bronze, silver, gold zones)
- **Monitoring/analysis** via Streamlit dashboard
- **Workflow automation** using Airflow (ETL orchestration & notebook triggering)

This end-to-end pipeline covers ingestion, storage, transformation, and visualizationâ€”showcasing data architecture, engineering, and DevOps capabilities.

---

### ðŸ—ï¸ Architecture

```plaintext
                        +------------------+
                        |  RabbitMQ        |
                        | (Producer/Queue) |
                        +--------+---------+
                                 |
                                 v
                         +-------+--------+
                         | RabbitMQ       |
                         | Consumer       |
                         | (Writes Bronze |
                         |  S3 layer)     |
                         +-------+--------+
                                 |
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€Batch Notebook / Airflowâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â†“                                                   â†“
+---------------+        +----------------+        +------------------+
|  Bronze (S3)  | -----> |  Silver (S3)   | -----> |  Gold (S3)       |
+---------------+        +----------------+        +------------------+
     Raw data           Cleaned/formatted        Aggregated results
     (Json & delta)                (delta)                   (delta)

        â†‘
        |
+---------------+
| Postgres      |
| (source data) |
+---------------+

        â†“
+----------------+
|  Streamlit     |
|  Dashboard     |
+----------------+

        â†“
+----------------+
|  Jupyter       |
|  Notebook UI   |
+----------------+

```

---

### âš™ï¸ Project Structure

```
â”œâ”€â”€ airflow/                 â†’ Airflow DAGs and Docker build
â”œâ”€â”€ jupyter/                 â†’ Jupyter notebooks and Dockerfile
â”œâ”€â”€ postgres-producer/       â†’ Ingests static data into Postgres
â”œâ”€â”€ rabbitmq/producer/       â†’ Real-time event producer
â”œâ”€â”€ rabbitmq/consumer/       â†’ Consumer writing to S3 (bronze)
â”œâ”€â”€ dashboard/               â†’ Streamlit dashboard
â”œâ”€â”€ local-s3/                â†’ Auto created local folder which is mounted as S3Mock
â”œâ”€â”€ docker-compose.yml       â†’ Multi-service orchestration
â”œâ”€â”€ start_env.sh             â†’ Sets up the environment
â”œâ”€â”€ run_postgres_producer.sh â†’ Loads Postgres sample data
â”œâ”€â”€ run_realtime_pipeline.sh â†’ Starts RabbitMQ producer/consumer
```

---

### ðŸš€ Run Instructions

#### 1. Start full environment

```bash
./start_env.sh
```

> ðŸ§¼ This cleans and rebuilds everything (Airflow, RabbitMQ, Postgres, Jupyter, Streamlit, S3Mock)

#### 2. Load batch source data into Postgres

```bash
./run_postgres_producer.sh
```

#### 3. Run real-time producer and consumer

```bash
./run_realtime_pipeline.sh
```

#### 4. Run batch pipeline manually

- Option A: Trigger the Airflow DAG: `run_postgres_etl_notebook`:[http://localhost:8080/](http://localhost:8080/)

  Username/Password: admin/admin

- Option B: Open Jupyter: [http://localhost:8888/](http://localhost:8888/)

  Token: `test-token`

  Then run the notebook: `batch.ipynb`

#### 5. Run real time pipeline manually

- Open Jupyter: [http://localhost:8888/](http://localhost:8888/)

  Token: `test-token`

  Then run the notebook: `bronze_silver_preprocessing.ipynb`, and `gold_aggregate_dashboard.ipynb`

#### 6. Visualize results

- Open Streamlit dashboard: [http://localhost:8501/](http://localhost:8501/)
- In the home page, you would see User Activity per Event Type per Day in real time

---

### ðŸ”§ Notes

- S3 paths: `/mnt/s3mock/bronze`, `/mnt/s3mock/silver`, `/mnt/s3mock/gold`
- Delta tables are written via Spark with `delta-spark==2.4.0`
- Airflow DAGs are stored in `./airflow/dags`
- The Jupyter container runs on ports `8888` (token access) and `9999` (optional extensions)
